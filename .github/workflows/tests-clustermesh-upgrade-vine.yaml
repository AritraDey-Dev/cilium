name: Cilium Cluster Mesh IPsec VinE upgrade (ci-clustermesh-ipsec-vine)

# Any change in triggers needs to be reflected in the concurrency group.
on:
  workflow_dispatch:
    inputs:
      PR-number:
        description: "Pull request number."
        required: true
      context-ref:
        description: "Context in which the workflow runs. If PR is from a fork, will be the PR target branch (general case). If PR is NOT from a fork, will be the PR branch itself (this allows committers to test changes to workflows directly from PRs)."
        required: true
      SHA:
        description: "SHA under test (head of the PR branch)."
        required: true
      extra-args:
        description: "[JSON object] Arbitrary arguments passed from the trigger comment via regex capture group. Parse with 'fromJson(inputs.extra-args).argName' in workflow."
        required: false
        default: '{}'

  push:
    branches:
      - main
      - ft/main/**
      - 'renovate/main-**'
      - ldelossa/ipsec-vxlan-in-esp-upgrade-workflow
    paths-ignore:
      - 'Documentation/**'

# By specifying the access of one of the scopes, all of those that are not
# specified are set to 'none'.
permissions:
  # To read actions state with catchpoint/workflow-telemetry-action
  actions: read
  # To be able to access the repository with actions/checkout
  contents: read
  # To allow retrieving information from the PR API
  pull-requests: read
  # To be able to set commit status
  statuses: write

concurrency:
  # Structure:
  # - Workflow name
  # - Event type
  # - A unique identifier depending on event type:
  #   - push: SHA
  #   - workflow_dispatch: PR number
  #
  # This structure ensures a unique concurrency group name is generated for each
  # type of testing, such that re-runs will cancel the previous run.
  group: |
    ${{ github.workflow }}
    ${{ github.event_name }}
    ${{
      (github.event_name == 'push' && github.sha) ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.PR-number)
    }}
  cancel-in-progress: true

env:
  clusterName1: cluster1
  clusterName2: cluster2
  contextName1: kind-cluster1
  contextName2: kind-cluster2

jobs:
  echo-inputs:
    if: ${{ github.event_name == 'workflow_dispatch' }}
    name: Echo Workflow Dispatch Inputs
    runs-on: ubuntu-24.04
    steps:
      - name: Echo Workflow Dispatch Inputs
        run: |
          echo '${{ tojson(inputs) }}'

  commit-status-start:
    name: Commit Status Start
    runs-on: ubuntu-24.04
    steps:
      - name: Set initial commit status
        uses: myrotvorets/set-commit-status-action@3730c0a348a2ace3c110851bed53331bc6406e9f # v2.0.1
        with:
          sha: ${{ inputs.SHA || github.sha }}

  upgrade-and-downgrade:
    name: "Upgrade and Downgrade IPsec VinE Test"
    runs-on: ${{ vars.GH_RUNNER_EXTRA_POWER_UBUNTU_LATEST || 'ubuntu-latest' }}
    timeout-minutes: 60
    env:
      job_name: "Installation and Connectivity Test"

    strategy:
      fail-fast: false
      matrix:
        include:
          - name: '3'
            encryption: 'ipsec'
            kube-proxy: 'iptables'
            external-kvstore: false
            max-connected-clusters: 255
            cm-auth-mode: 'cluster'

    steps:
      - name: Collect Workflow Telemetry
        uses: catchpoint/workflow-telemetry-action@94c3c3d9567a0205de6da68a76c428ce4e769af1 # v2.0.0
        with:
          comment_on_pr: false

      - name: Checkout context ref (trusted)
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: ${{ inputs.context-ref || github.sha }}
          persist-credentials: false

      - name: Set Environment Variables
        uses: ./.github/actions/set-env-variables

      - name: Set up newest settings
        id: newest-vars
        uses: ./.github/actions/helm-default
        with:
          image-tag: ${{ inputs.SHA || github.sha }}
          chart-dir: ./untrusted/cilium-newest/install/kubernetes/cilium

      - name: Set up job variables
        id: vars
        run: |
          CILIUM_DOWNGRADE_VERSION=$(contrib/scripts/print-downgrade-version.sh stable)
          echo "downgrade_version=${CILIUM_DOWNGRADE_VERSION}" >> $GITHUB_OUTPUT
          echo "using downgrade version ${CILIUM_DOWNGRADE_VERSION}"

          # * Monitor aggregation is set to medium to avoid the performance penalty
          #   in the testing environment due to the relatively high traffic load.
          # * We explicitly configure the IPAM mode to prevent it from being
          #   reset to the default value on upgrade/downgrade due to --reset-values.
          # * We explicitly configure the sync timeout to a higher value to
          #   give enough time to the clustermesh-apiserver to restart after
          #   the upgrade/downgrade before that agents regenerate the endpoints.
          # * We configure the maximum number of unavailable agents to 1 to slow
          #   down the rollout process and highlight possible connection disruption
          #   occurring in the meanwhile.
          CILIUM_INSTALL_DEFAULTS=" \
            --set=debug.enabled=true \
            --set=bpf.monitorAggregation=medium \
            --set=hubble.enabled=true \
            --set=routingMode=tunnel \
            --set=tunnelProtocol=vxlan \
            --set=ipv4.enabled=true \
            --set=ipv6.enabled=true \
            --set=kubeProxyReplacement=${{ matrix.kube-proxy == 'none' }} \
            --set=bpf.masquerade=${{ matrix.kube-proxy == 'none' }} \
            --set=ipam.mode=kubernetes \
            --set=operator.replicas=1 \
            --set=updateStrategy.rollingUpdate.maxUnavailable=1 \
            --set=clustermesh.useAPIServer=${{ !matrix.external-kvstore }} \
            --set=clustermesh.maxConnectedClusters=${{ matrix.max-connected-clusters }} \
            --set=clustermesh.config.enabled=true \
            --set=extraConfig.clustermesh-sync-timeout=10m \
            --set=clustermesh.apiserver.readinessProbe.periodSeconds=1 \
            --set=clustermesh.apiserver.kvstoremesh.readinessProbe.periodSeconds=1 \
            --set=clustermesh.apiserver.updateStrategy.rollingUpdate.maxSurge=1 `# Use surge update strategy to enable clients to failover` \
            --set=clustermesh.apiserver.updateStrategy.rollingUpdate.maxUnavailable=0 \
            --set=clustermesh.apiserver.tls.authMode=${{ matrix.cm-auth-mode }} \
          "

          # Leak detection for encryption tests do not function correctly for
          # mixed cluster testing.
          #
          # Therefore, run all tests accept for encryption leak detection and
          # ensure connection disruption does not occur.
          CONNECTIVITY_TEST_DEFAULTS=" \
            --hubble=false \
            --test='!encryption' \
            --collect-sysdump-on-failure"

          CILIUM_INSTALL_ENCRYPTION=""
          if [ "${{ matrix.encryption }}" != "disabled" ]; then
            CILIUM_INSTALL_ENCRYPTION=" \
              --set=encryption.enabled=true \
              --set=encryption.type=${{ matrix.encryption }}"
          fi

          echo "cilium_install_defaults=${CILIUM_INSTALL_DEFAULTS} ${CILIUM_INSTALL_ENCRYPTION}" >> $GITHUB_OUTPUT
          echo "connectivity_test_defaults=${CONNECTIVITY_TEST_DEFAULTS}" >> $GITHUB_OUTPUT

      - name: Generate Kind configuration files
        run: |
          PODCIDR=10.242.0.0/16,fd00:10:242::/48 \
            SVCCIDR=10.243.0.0/16,fd00:10:243::/112 \
            IPFAMILY=dual \
            KUBEPROXYMODE=${{ matrix.kube-proxy }} \
            envsubst < ./.github/kind-config.yaml.tmpl > ./.github/kind-config-cluster1.yaml

          PODCIDR=10.244.0.0/16,fd00:10:244::/48 \
            SVCCIDR=10.245.0.0/16,fd00:10:245::/112 \
            IPFAMILY=dual \
            KUBEPROXYMODE=${{ matrix.kube-proxy }} \
            envsubst < ./.github/kind-config.yaml.tmpl > ./.github/kind-config-cluster2.yaml

      - name: Create Kind cluster 1
        uses: helm/kind-action@9fdad0686e6f19fcd572f62516f5e0436f562ee7 # v1.10.0
        with:
          cluster_name: ${{ env.clusterName1 }}
          version: ${{ env.KIND_VERSION }}
          node_image: ${{ env.KIND_K8S_IMAGE }}
          kubectl_version: ${{ env.KIND_K8S_VERSION }}
          config: ./.github/kind-config-cluster1.yaml
          wait: 0 # The control-plane never becomes ready, since no CNI is present

      - name: Create Kind cluster 2
        uses: helm/kind-action@9fdad0686e6f19fcd572f62516f5e0436f562ee7 # v1.10.0
        with:
          cluster_name: ${{ env.clusterName2 }}
          version: ${{ env.KIND_VERSION }}
          node_image: ${{ env.KIND_K8S_IMAGE }}
          kubectl_version: ${{ env.KIND_K8S_VERSION }}
          config: ./.github/kind-config-cluster2.yaml
          wait: 0 # The control-plane never becomes ready, since no CNI is present

      - name: Install Cilium CLI
        uses: cilium/cilium-cli@c52e8c38e6d6235bd8e6e961199a984275547d6f # v0.16.22
        with:
          skip-build: ${{ env.CILIUM_CLI_SKIP_BUILD }}
          image-repo: ${{ env.CILIUM_CLI_IMAGE_REPO }}
          image-tag: ${{ inputs.SHA || github.sha }}

      # Make sure that coredns uses IPv4-only upstream DNS servers also in case of clusters
      # with IP family dual, since IPv6 ones are not reachable and cause spurious failures.
      # Additionally, this is also required to workaround
      # https://github.com/cilium/cilium/issues/23283#issuecomment-1597282247.
      - name: Configure the coredns nameservers
        run: |
          COREDNS_PATCH="
          spec:
            template:
              spec:
                dnsPolicy: None
                dnsConfig:
                  nameservers:
                  - 8.8.4.4
                  - 8.8.8.8
          "

          kubectl --context ${{ env.contextName1 }} patch deployment -n kube-system coredns --patch="$COREDNS_PATCH"
          kubectl --context ${{ env.contextName2 }} patch deployment -n kube-system coredns --patch="$COREDNS_PATCH"


      - name: Create the IPSec secret in both clusters
        if: matrix.encryption == 'ipsec'
        run: |
          SECRET="3+ rfc4106(gcm(aes)) $(openssl rand -hex 20) 128"
          kubectl --context ${{ env.contextName1 }} create -n kube-system secret generic cilium-ipsec-keys --from-literal=keys="${SECRET}"
          kubectl --context ${{ env.contextName2 }} create -n kube-system secret generic cilium-ipsec-keys --from-literal=keys="${SECRET}"

      - name: Set clustermesh connection parameters
        id: clustermesh-vars
        run: |
          # Let's retrieve in advance the parameters to mesh the two clusters, so
          # that we don't need to do that through the CLI in a second step, as it
          # would be reset during upgrade (as we are resetting the values).

          # Explicitly configure the NodePorts to make sure that they are different
          # in each cluster, to workaround #24692
          PORT1=32379
          PORT2=32380

          CILIUM_INSTALL_CLUSTER1=" \
            --set cluster.name=${{ env.clusterName1 }} \
            --set cluster.id=1 \
            --set clustermesh.apiserver.service.nodePort=$PORT1 \
          "

          CILIUM_INSTALL_CLUSTER2=" \
            --set cluster.name=${{ env.clusterName2 }} \
            --set cluster.id=${{ matrix.max-connected-clusters }} \
            --set clustermesh.apiserver.service.nodePort=$PORT2 \
          "

          CILIUM_INSTALL_COMMON=" \
            --set clustermesh.config.clusters[0].name=${{ env.clusterName1 }} \
            --set clustermesh.config.clusters[1].name=${{ env.clusterName2 }} \
          "

          if [ "${{ matrix.external-kvstore }}" == "true" ]; then
            CILIUM_INSTALL_COMMON="$CILIUM_INSTALL_COMMON \
              ${{ steps.kvstore.outputs.cilium_install_clustermesh }}"
          else
            IP1=$(kubectl --context ${{ env.contextName1 }} get nodes \
              ${{ env.clusterName1 }}-worker -o wide --no-headers | awk '{ print $6 }')
            IP2=$(kubectl --context ${{ env.contextName2 }} get nodes \
              ${{ env.clusterName2 }}-worker -o wide --no-headers | awk '{ print $6 }')

            CILIUM_INSTALL_COMMON="$CILIUM_INSTALL_COMMON \
              --set clustermesh.config.clusters[0].ips={$IP1} \
              --set clustermesh.config.clusters[0].port=$PORT1 \
              --set clustermesh.config.clusters[1].ips={$IP2} \
              --set clustermesh.config.clusters[1].port=$PORT2 \
            "
          fi

          echo cilium_install_cluster1="$CILIUM_INSTALL_CLUSTER1 $CILIUM_INSTALL_COMMON" >> $GITHUB_OUTPUT
          echo cilium_install_cluster2="$CILIUM_INSTALL_CLUSTER2 $CILIUM_INSTALL_COMMON" >> $GITHUB_OUTPUT

      # Warning: since this is a privileged workflow, subsequent workflow job
      # steps must take care not to execute untrusted code.
      - name: Checkout pull request branch (NOT TRUSTED)
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: ${{ steps.newest-vars.outputs.sha }}
          persist-credentials: false
          path: untrusted/cilium-newest
          sparse-checkout: |
            install/kubernetes/cilium

      - name: Checkout ${{ steps.vars.outputs.downgrade_version }} branch
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: ${{ steps.vars.outputs.downgrade_version }}
          persist-credentials: false
          path: untrusted/cilium-downgrade
          sparse-checkout: |
            install/kubernetes/cilium

      - name: Retrieve downgrade target SHA
        id: downgrade-branch
        run: |
          echo "sha=$(cd untrusted/cilium-downgrade && git rev-parse HEAD)" >> $GITHUB_OUTPUT

      - name: Set up downgrade settings
        id: downgrade-vars
        uses: ./.github/actions/helm-default
        with:
          image-tag: ${{ steps.downgrade-branch.outputs.sha }}
          chart-dir: ./untrusted/cilium-downgrade/install/kubernetes/cilium

      - name: Wait for images to be available (newest)
        timeout-minutes: 10
        shell: bash
        run: |
          for image in cilium-ci operator-generic-ci clustermesh-apiserver-ci ; do
            until docker manifest inspect quay.io/${{ env.QUAY_ORGANIZATION_DEV }}/$image:${{ steps.newest-vars.outputs.sha }} &> /dev/null; do sleep 45s; done
          done

      - name: Wait for images to be available (downgrade)
        timeout-minutes: 10
        shell: bash
        run: |
          for image in cilium-ci operator-generic-ci clustermesh-apiserver-ci ; do
            until docker manifest inspect quay.io/${{ env.QUAY_ORGANIZATION_DEV }}/$image:${{ steps.downgrade-branch.outputs.sha }} &> /dev/null; do sleep 45s; done
          done

      #
      # Start with two downgraded clusters and perform an upgrade of cluster 1
      # to VinE
      #
      # Do this so we understand if the upgrade process itself breaks the data
      # path.
      #
      - name: Install Cilium in Cluster1[Downgrade]
        id: install-cilium-cluster1
        env:
          KVSTORE_ID: 1
        run: |
          cilium --context ${{ env.contextName1 }} install \
            ${{ steps.downgrade-vars.outputs.cilium_install_defaults }} \
            ${{ steps.vars.outputs.cilium_install_defaults }} \
            ${{ steps.kvstore.outputs.cilium_install_kvstore }} \
            ${{ steps.clustermesh-vars.outputs.cilium_install_cluster1 }}

      - name: Copy the Cilium CA secret to cluster2, as they must match
        if: ${{ !matrix.external-kvstore }}
        run: |
          kubectl --context ${{ env.contextName1 }} get secret -n kube-system cilium-ca -o yaml |
            kubectl --context ${{ env.contextName2 }} create -f -

      - name: Install Cilium in Cluster2[Downgrade]
        env:
          KVSTORE_ID: 2
        run: |
          cilium --context ${{ env.contextName2 }} install \
            ${{ steps.downgrade-vars.outputs.cilium_install_defaults }} \
            ${{ steps.vars.outputs.cilium_install_defaults }} \
            ${{ steps.kvstore.outputs.cilium_install_kvstore }} \
            ${{ steps.clustermesh-vars.outputs.cilium_install_cluster2 }}

      - name: Wait for cluster mesh status to be ready
        run: |
          cilium --context ${{ env.contextName1 }} status --wait --wait-duration=10m
          cilium --context ${{ env.contextName2 }} status --wait --wait-duration=10m
          cilium --context ${{ env.contextName1 }} clustermesh status --wait --wait-duration=5m
          cilium --context ${{ env.contextName2 }} clustermesh status --wait --wait-duration=5m

      - name: Setup conn-disrupt-test before upgrading
        run: |
          cilium --context ${{ env.contextName1 }} connectivity test \
            --multi-cluster=${{ env.contextName2 }} --hubble=false \
            --include-conn-disrupt-test --conn-disrupt-test-setup \
            --conn-disrupt-test-restarts-path "./cilium-conn-disrupt-restarts" \
            --conn-disrupt-test-xfrm-errors-path "./cilium-conn-disrupt-xfrm-errors" \
            --conn-disrupt-dispatch-interval 0ms

      - name: Setup tcpdump (before upgrade)
        run: |
          for hostpod in $(kubectl --context kind-cluster1 -n cilium-test-1 get po -l kind=host-netns -o jsonpath='{.items[*].metadata.name}'); do
            kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -w /tmp/$hostpod.einv.pcap -ni eth0 -Q out 'udp[8+8+14+9] == 50' &
            kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -w /tmp/$hostpod.plain.pcap -ni eth0 -Q out 'udp[8+8+14+9] != 50 and !esp' &
          done
          for hostpod in $(kubectl --context kind-cluster2 -n cilium-test-1 get po -l kind=host-netns -o jsonpath='{.items[*].metadata.name}'); do
            kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -w /tmp/$hostpod.vine.pcap -ni eth0 -Q out 'esp' &
            kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -w /tmp/$hostpod.plain.pcap -ni eth0 -Q out 'udp[8+8+14+9] != 50 and !esp' &
          done

      - name: Upgrade Cluster1[VinE]
        env:
          KVSTORE_ID: 1
        run: |
          cilium --context ${{ env.contextName1 }} upgrade --reset-values \
            ${{ steps.newest-vars.outputs.cilium_install_defaults }} \
            ${{ steps.vars.outputs.cilium_install_defaults }} \
            ${{ steps.kvstore.outputs.cilium_install_kvstore }} \
            ${{ steps.clustermesh-vars.outputs.cilium_install_cluster1 }}

      - name: Wait for cluster mesh status to be ready
        run: |
          cilium --context ${{ env.contextName1 }} status --wait --wait-duration=10m
          cilium --context ${{ env.contextName2 }} status --wait --wait-duration=10m
          cilium --context ${{ env.contextName1 }} clustermesh status --wait --wait-duration=5m
          cilium --context ${{ env.contextName2 }} clustermesh status --wait --wait-duration=5m

      - name: Check conn-disrupt-test after upgrading
        run: |
          cilium --context ${{ env.contextName1 }} connectivity test \
            --multi-cluster=${{ env.contextName2 }} --hubble=false \
            --include-conn-disrupt-test --include-conn-disrupt-test-ns-traffic \
            --conn-disrupt-test-restarts-path "./cilium-conn-disrupt-restarts" \
            --conn-disrupt-test-xfrm-errors-path "./cilium-conn-disrupt-xfrm-errors" \
            --test 'no-interrupted-connections'

      - name: Check tcpdump output (after upgrade)
        run: |
          #set -x
          for context in kind-cluster1 kind-cluster2; do
            for hostpod in $(kubectl --context $context -n cilium-test-1 get po -l kind=host-netns -o jsonpath='{.items[*].metadata.name}'); do
              kubectl --context $context -n cilium-test-1 exec $hostpod -- pkill tcpdump
              kubectl --context $context -n cilium-test-1 exec $hostpod -- ls -lh /tmp/
            done
          done

          # During upgrade there will be mixed types of traffic
          #
          #for hostpod in $(kubectl --context kind-cluster1 -n cilium-test-1 get po -l kind=host-netns -o jsonpath='{.items[*].metadata.name}'); do
          #  if [ $(kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.einv.pcap 2>/dev/null | wc -l) -gt 0 ]; then
          #    echo "EinV found in cluster1(1.18)" >&2
          #    kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.einv.pcap
          #    exit 1
          #  fi
          #done

          for hostpod in $(kubectl --context kind-cluster2 -n cilium-test-1 get po -l kind=host-netns -o jsonpath='{.items[*].metadata.name}'); do
            if [ $(kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.einv.pcap 2>/dev/null | wc -l) -gt 0 ]; then
              echo "VinE found in cluster2(1.17)" >&2
              tcpdump -nn -r /tmp/$hostpod.vine.pcap
              exit 1
            fi
          done

          # ========================

          internal_ips=""
          for context in kind-cluster1 kind-cluster2; do
            for node in $(kubectl --context $context get ciliumnodes.cilium.io --no-headers -o custom-columns=":metadata.name"); do
              for ip in $(kubectl --context $context get ciliumnodes.cilium.io "$node" -o jsonpath='{range .spec.addresses[?(@.type=="CiliumInternalIP")]}{.ip}{"\n"}{end}'); do
                if [ -z "$internal_ips" ]; then
                  internal_ips="$ip"
                else
                  internal_ips="${internal_ips}|$ip"
                fi
              done
            done
          done

          echo "Collected internal_ips: $internal_ips"
          export internal_ips=$internal_ips

          # ========================

          for hostpod in $(kubectl --context kind-cluster1 -n cilium-test-1 get po -l kind=host-netns -o jsonpath='{.items[*].metadata.name}'); do
            if [ -n "$(kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'src net 10.242.0.0/16 or dst net 10.244.0.0/16' 2>/dev/null | grep -Pv $internal_ips)" ]; then
              echo "Plain-text traffic with Pod IP (IPv4) found in kind-cluster1($hostpod)" >&2
              kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'src net 10.242.0.0/16 or dst net 10.244.0.0/16' 2>/dev/null | grep -Pv $internal_ips
              exit 1
            fi

            if [ -n "$(kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'udp and port 8472 and udp[28:2] == 0x0800 and ((udp[42:4] & 0xffff0000 == 0x0af20000) or (udp[46:4] & 0xffff0000 == 0x0af40000))' 2>/dev/null | grep -Pv $internal_ips | grep -v overlay)" ]; then
              echo "Plain-text traffic with Pod IP (IPv4 in VxLAN) found in kind-cluster1($hostpod)" >&2
              kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'udp and port 8472 and udp[28:2] == 0x0800 and ((udp[42:4] & 0xffff0000 == 0x0af20000) or (udp[46:4] & 0xffff0000 == 0x0af40000))' 2>/dev/null | grep -Pv $internal_ips | grep -v overlay
              exit 1
            fi

            if [ -n "$(kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'src net fd00:10:242::/48 or dst net fd00:10:244::/48' 2>/dev/null | grep -Pv $internal_ips)" ]; then
              echo "Plain-text traffic with Pod IP (IPv6) found in kind-cluster1($hostpod)" >&2
              kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'src net fd00:10:242::/48 or dst net fd00:10:244::/48' 2>/dev/null | grep -Pv $internal_ips
              exit 1
            fi

            if [ -n "$(kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'udp and port 8472 and udp[28:2] == 0x86dd and ((udp[38:4] == 0xfd000010 and (udp[42:4] & 0xffff0000) == 0x02420000) or (udp[54:4] == 0xfd000010 and (udp[58:4] & 0xffff0000) == 0x02440000))' 2>/dev/null | grep -Pv $internal_ips | grep -v overlay)" ]; then
              echo "Plain-text traffic with Pod IP (IPv6 in VxLAN) found in kind-cluster1($hostpod)" >&2
              kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'udp and port 8472 and udp[28:2] == 0x86dd and ((udp[38:4] == 0xfd000010 and (udp[42:4] & 0xffff0000) == 0x02420000) or (udp[54:4] == 0xfd000010 and (udp[58:4] & 0xffff0000) == 0x02440000))' 2>/dev/null | grep -Pv $internal_ips | grep -v overlay
              exit 1
            fi
          done

          # ========================

          for hostpod in $(kubectl --context kind-cluster2 -n cilium-test-1 get po -l kind=host-netns -o jsonpath='{.items[*].metadata.name}'); do
            if [ -n "$(kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'src net 10.244.0.0/16 or dst net 10.242.0.0/16' 2>/dev/null | grep -Pv $internal_ips)" ]; then
              echo "Plain-text traffic with Pod IP (IPv4) found in kind-cluster2($hostpod)" >&2
              kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'src net 10.244.0.0/16 or dst net 10.242.0.0/16' 2>/dev/null | grep -Pv $internal_ips
              exit 1
            fi

            if [ -n "$(kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'udp and port 8472 and udp[28:2] == 0x0800 and ((udp[42:4] & 0xffff0000 == 0x0af40000) or (udp[46:4] & 0xffff0000 == 0x0af20000))' 2>/dev/null | grep -Pv $internal_ips | grep -v overlay)" ]; then
              echo "Plain-text traffic with Pod IP (IPv4 in VxLAN) found in kind-cluster2($hostpod)" >&2
              kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'udp and port 8472 and udp[28:2] == 0x0800 and ((udp[42:4] & 0xffff0000 == 0x0af40000) or (udp[46:4] & 0xffff0000 == 0x0af20000))' 2>/dev/null | grep -Pv $internal_ips | grep -v overlay
              exit 1
            fi

            if [ -n "$(kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'src net fd00:10:244::/48 or dst net fd00:10:242::/48' 2>/dev/null | grep -Pv $internal_ips)" ]; then
              echo "Plain-text traffic with Pod IP (IPv6) found in kind-cluster2($hostpod)" >&2
              kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'src net fd00:10:244::/48 or dst net fd00:10:242::/48' 2>/dev/null | grep -Pv $internal_ips
              exit 1
            fi

            if [ -n "$(kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'udp and port 8472 and udp[28:2] == 0x86dd and ((udp[38:4] == 0xfd000010 and (udp[42:4] & 0xffff0000) == 0x02440000) or (udp[54:4] == 0xfd000010 and (udp[58:4] & 0xffff0000) == 0x02420000))' 2>/dev/null | grep -Pv $internal_ips | grep -v overlay)" ]; then
              echo "Plain-text traffic with Pod IP (IPv6 in VxLAN) found in kind-cluster2($hostpod)" >&2
              kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'udp and port 8472 and udp[28:2] == 0x86dd and ((udp[38:4] == 0xfd000010 and (udp[42:4] & 0xffff0000) == 0x02440000) or (udp[54:4] == 0xfd000010 and (udp[58:4] & 0xffff0000) == 0x02420000))' 2>/dev/null | grep -Pv $internal_ips | grep -v overlay
              exit 1
            fi
          done

      #
      # Cluster 1 is now VinE and Cluster 2 is now the downgraded cluster
      # Run cross-cluster connectivity tests
      #

      - name: Make JUnit report directory
        run: |
          mkdir -p cilium-junits

      - name: Setup connectivity pods
        run: |
          cilium --context ${{ env.contextName1 }} connectivity test \
            --multi-cluster=${{ env.contextName2 }} \
            --test 'skipall'

      - name: Setup tcpdump
        run: |
          #set -x
          # on cluster1, we capture EinV traffic and plain-text traffic; on cluster2, we capture VinE traffic and plain-text traffic.
          # The expectations are:
          # 1. EinV on cluster1: no output
          # 2. VinE on cluster2: no output
          # 3. plain-text on both: mustn't have pod IP (outer and inner) in the packet
          for hostpod in $(kubectl --context kind-cluster1 -n cilium-test-1 get po -l kind=host-netns -o jsonpath='{.items[*].metadata.name}'); do
            kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -w /tmp/$hostpod.einv.pcap -ni eth0 -Q out 'udp[8+8+14+9] == 50' &
            kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -w /tmp/$hostpod.plain.pcap -ni eth0 -Q out 'udp[8+8+14+9] != 50 and !esp' &
          done
          for hostpod in $(kubectl --context kind-cluster2 -n cilium-test-1 get po -l kind=host-netns -o jsonpath='{.items[*].metadata.name}'); do
            kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -w /tmp/$hostpod.vine.pcap -ni eth0 -Q out 'esp' &
            kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -w /tmp/$hostpod.plain.pcap -ni eth0 -Q out 'udp[8+8+14+9] != 50 and !esp' &
          done

      - name: Run connectivity test - Cluster1[VinE]<->Cluster2[Downgrade] (${{ join(matrix.*, ', ') }})
        run: |
          cilium --context ${{ env.contextName1 }} connectivity test \
            --multi-cluster=${{ env.contextName2 }} \
            ${{ steps.vars.outputs.connectivity_test_defaults }} \
            --junit-file "cilium-junits/${{ env.job_name }} - pre-upgrade (${{ join(matrix.*, ', ') }}).xml" \
            --junit-property github_job_step="Run tests pre-upgrade (${{ join(matrix.*, ', ') }})"

          # Create pods which establish long lived connections. They will be used by
          # subsequent connectivity tests with --include-conn-disrupt-test to catch any
          # interruption in such flows.
          cilium --context ${{ env.contextName1 }} connectivity test \
            --multi-cluster=${{ env.contextName2 }} --hubble=false \
            --include-conn-disrupt-test --conn-disrupt-test-setup \
            --conn-disrupt-test-restarts-path "./cilium-conn-disrupt-restarts" \
            --conn-disrupt-test-xfrm-errors-path "./cilium-conn-disrupt-xfrm-errors" \
            --conn-disrupt-dispatch-interval 0ms

      - name: Features tested on cluster 1 Cluster1[VinE]<->Cluster2[Downgrade]
        uses: ./.github/actions/feature-status
        with:
          cilium-cli: "cilium --context ${{ env.contextName1 }}"
          title: "Summary of all features tested on cluster 1"
          json-filename: "${{ env.job_name }} (${{ join(matrix.*, ', ') }}) - cluster 1"

      - name: Features tested on cluster 2 Cluster1[VinE]<->Cluster2[Downgrade]
        uses: ./.github/actions/feature-status
        with:
          cilium-cli: "cilium --context ${{ env.contextName2 }}"
          title: "Summary of all features tested on cluster 2"
          json-filename: "${{ env.job_name }} (${{ join(matrix.*, ', ') }}) - cluster 2"

      - name: Check tcpdump output
        run: |
          #set -x
          for context in kind-cluster1 kind-cluster2; do
            for hostpod in $(kubectl --context $context -n cilium-test-1 get po -l kind=host-netns -o jsonpath='{.items[*].metadata.name}'); do
              kubectl --context $context -n cilium-test-1 exec $hostpod -- pkill tcpdump
              kubectl --context $context -n cilium-test-1 exec $hostpod -- ls -lh /tmp/
            done
          done

          for hostpod in $(kubectl --context kind-cluster1 -n cilium-test-1 get po -l kind=host-netns -o jsonpath='{.items[*].metadata.name}'); do
            if [ $(kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.einv.pcap 2>/dev/null | wc -l) -gt 0 ]; then
              echo "EinV found in cluster1(1.18)" >&2
              kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.einv.pcap
              exit 1
            fi
          done

          for hostpod in $(kubectl --context kind-cluster2 -n cilium-test-1 get po -l kind=host-netns -o jsonpath='{.items[*].metadata.name}'); do
            if [ $(kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.einv.pcap 2>/dev/null | wc -l) -gt 0 ]; then
              echo "VinE found in cluster2(1.17)" >&2
              tcpdump -nn -r /tmp/$hostpod.vine.pcap
              exit 1
            fi
          done

          # ========================

          internal_ips=""
          for context in kind-cluster1 kind-cluster2; do
            for node in $(kubectl --context $context get ciliumnodes.cilium.io --no-headers -o custom-columns=":metadata.name"); do
              for ip in $(kubectl --context $context get ciliumnodes.cilium.io "$node" -o jsonpath='{range .spec.addresses[?(@.type=="CiliumInternalIP")]}{.ip}{"\n"}{end}'); do
                if [ -z "$internal_ips" ]; then
                  internal_ips="$ip"
                else
                  internal_ips="${internal_ips}|$ip"
                fi
              done
            done
          done

          echo "Collected internal_ips: $internal_ips"
          export internal_ips=$internal_ips

          # ========================

          for hostpod in $(kubectl --context kind-cluster1 -n cilium-test-1 get po -l kind=host-netns -o jsonpath='{.items[*].metadata.name}'); do
            if [ -n "$(kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'src net 10.242.0.0/16 or dst net 10.244.0.0/16' 2>/dev/null | grep -Pv $internal_ips)" ]; then
              echo "Plain-text traffic with Pod IP (IPv4) found in kind-cluster1($hostpod)" >&2
              kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'src net 10.242.0.0/16 or dst net 10.244.0.0/16' 2>/dev/null | grep -Pv $internal_ips
              exit 1
            fi

            if [ -n "$(kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'udp and port 8472 and udp[28:2] == 0x0800 and ((udp[42:4] & 0xffff0000 == 0x0af20000) or (udp[46:4] & 0xffff0000 == 0x0af40000))' 2>/dev/null | grep -Pv $internal_ips | grep -v overlay)" ]; then
              echo "Plain-text traffic with Pod IP (IPv4 in VxLAN) found in kind-cluster1($hostpod)" >&2
              kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'udp and port 8472 and udp[28:2] == 0x0800 and ((udp[42:4] & 0xffff0000 == 0x0af20000) or (udp[46:4] & 0xffff0000 == 0x0af40000))' 2>/dev/null | grep -Pv $internal_ips | grep -v overlay
              exit 1
            fi

            if [ -n "$(kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'src net fd00:10:242::/48 or dst net fd00:10:244::/48' 2>/dev/null | grep -Pv $internal_ips)" ]; then
              echo "Plain-text traffic with Pod IP (IPv6) found in kind-cluster1($hostpod)" >&2
              kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'src net fd00:10:242::/48 or dst net fd00:10:244::/48' 2>/dev/null | grep -Pv $internal_ips
              exit 1
            fi

            if [ -n "$(kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'udp and port 8472 and udp[28:2] == 0x86dd and ((udp[38:4] == 0xfd000010 and (udp[42:4] & 0xffff0000) == 0x02420000) or (udp[54:4] == 0xfd000010 and (udp[58:4] & 0xffff0000) == 0x02440000))' 2>/dev/null | grep -Pv $internal_ips | grep -v overlay)" ]; then
              echo "Plain-text traffic with Pod IP (IPv6 in VxLAN) found in kind-cluster1($hostpod)" >&2
              kubectl --context kind-cluster1 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'udp and port 8472 and udp[28:2] == 0x86dd and ((udp[38:4] == 0xfd000010 and (udp[42:4] & 0xffff0000) == 0x02420000) or (udp[54:4] == 0xfd000010 and (udp[58:4] & 0xffff0000) == 0x02440000))' 2>/dev/null | grep -Pv $internal_ips | grep -v overlay
              exit 1
            fi
          done

          # ========================

          for hostpod in $(kubectl --context kind-cluster2 -n cilium-test-1 get po -l kind=host-netns -o jsonpath='{.items[*].metadata.name}'); do
            if [ -n "$(kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'src net 10.244.0.0/16 or dst net 10.242.0.0/16' 2>/dev/null | grep -Pv $internal_ips)" ]; then
              echo "Plain-text traffic with Pod IP (IPv4) found in kind-cluster2($hostpod)" >&2
              kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'src net 10.244.0.0/16 or dst net 10.242.0.0/16' 2>/dev/null | grep -Pv $internal_ips
              exit 1
            fi

            if [ -n "$(kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'udp and port 8472 and udp[28:2] == 0x0800 and ((udp[42:4] & 0xffff0000 == 0x0af40000) or (udp[46:4] & 0xffff0000 == 0x0af20000))' 2>/dev/null | grep -Pv $internal_ips | grep -v overlay)" ]; then
              echo "Plain-text traffic with Pod IP (IPv4 in VxLAN) found in kind-cluster2($hostpod)" >&2
              kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'udp and port 8472 and udp[28:2] == 0x0800 and ((udp[42:4] & 0xffff0000 == 0x0af40000) or (udp[46:4] & 0xffff0000 == 0x0af20000))' 2>/dev/null | grep -Pv $internal_ips | grep -v overlay
              exit 1
            fi

            if [ -n "$(kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'src net fd00:10:244::/48 or dst net fd00:10:242::/48' 2>/dev/null | grep -Pv $internal_ips)" ]; then
              echo "Plain-text traffic with Pod IP (IPv6) found in kind-cluster2($hostpod)" >&2
              kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'src net fd00:10:244::/48 or dst net fd00:10:242::/48' 2>/dev/null | grep -Pv $internal_ips
              exit 1
            fi

            if [ -n "$(kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'udp and port 8472 and udp[28:2] == 0x86dd and ((udp[38:4] == 0xfd000010 and (udp[42:4] & 0xffff0000) == 0x02440000) or (udp[54:4] == 0xfd000010 and (udp[58:4] & 0xffff0000) == 0x02420000))' 2>/dev/null | grep -Pv $internal_ips | grep -v overlay)" ]; then
              echo "Plain-text traffic with Pod IP (IPv6 in VxLAN) found in kind-cluster2($hostpod)" >&2
              kubectl --context kind-cluster2 -n cilium-test-1 exec $hostpod -- tcpdump -nn -r /tmp/$hostpod.plain.pcap 'udp and port 8472 and udp[28:2] == 0x86dd and ((udp[38:4] == 0xfd000010 and (udp[42:4] & 0xffff0000) == 0x02440000) or (udp[54:4] == 0xfd000010 and (udp[58:4] & 0xffff0000) == 0x02420000))' 2>/dev/null | grep -Pv $internal_ips | grep -v overlay
              exit 1
            fi
          done


      #
      # Cluster1[VinE]<->Cluster2[Downgrade] tests passed...
      # Upgrade to Cluster1[VinE]<->Cluster2[VinE] and test...
      #
      - name: Upgrade Cluster2[VinE]
        env:
          KVSTORE_ID: 2
        run: |
          cilium --context ${{ env.contextName2 }} upgrade --reset-values \
            ${{ steps.newest-vars.outputs.cilium_install_defaults }} \
            ${{ steps.vars.outputs.cilium_install_defaults }} \
            ${{ steps.kvstore.outputs.cilium_install_kvstore }} \
            ${{ steps.clustermesh-vars.outputs.cilium_install_cluster2 }}

      - name: Wait for cluster mesh status to be ready
        run: |
          cilium --context ${{ env.contextName1 }} status --wait --wait-duration=10m
          cilium --context ${{ env.contextName2 }} status --wait --wait-duration=10m
          cilium --context ${{ env.contextName1 }} clustermesh status --wait --wait-duration=5m
          cilium --context ${{ env.contextName2 }} clustermesh status --wait --wait-duration=5m

      - name: Run connectivity test - Cluster1[VinE]<->Cluster2[VinE] (${{ join(matrix.*, ', ') }})
        run: |
          cilium --context ${{ env.contextName1 }} connectivity test \
            --multi-cluster=${{ env.contextName2 }} \
            --include-conn-disrupt-test \
            --conn-disrupt-test-restarts-path "./cilium-conn-disrupt-restarts" \
            --conn-disrupt-test-xfrm-errors-path "./cilium-conn-disrupt-xfrm-errors" \
            ${{ steps.vars.outputs.connectivity_test_defaults }} \
            --junit-file "cilium-junits/${{ env.job_name }} - pre-upgrade (${{ join(matrix.*, ', ') }}).xml" \
            --junit-property github_job_step="Run tests pre-upgrade (${{ join(matrix.*, ', ') }})"

          # Create pods which establish long lived connections. They will be used by
          # subsequent connectivity tests with --include-conn-disrupt-test to catch any
          # interruption in such flows.
          cilium --context ${{ env.contextName1 }} connectivity test \
            --multi-cluster=${{ env.contextName2 }} --hubble=false \
            --include-conn-disrupt-test --conn-disrupt-test-setup \
            --conn-disrupt-test-restarts-path "./cilium-conn-disrupt-restarts" \
            --conn-disrupt-test-xfrm-errors-path "./cilium-conn-disrupt-xfrm-errors" \
            --conn-disrupt-dispatch-interval 0ms

      - name: Features tested on cluster 1 Cluster1[VinE]<->Cluster2[VinE]
        uses: ./.github/actions/feature-status
        with:
          cilium-cli: "cilium --context ${{ env.contextName1 }}"
          title: "Summary of all features tested on cluster 1"
          json-filename: "${{ env.job_name }} (${{ join(matrix.*, ', ') }}) - cluster 1"

      - name: Features tested on cluster 2 Cluster1[VinE]<->Cluster2[VinE]
        uses: ./.github/actions/feature-status
        with:
          cilium-cli: "cilium --context ${{ env.contextName2 }}"
          title: "Summary of all features tested on cluster 2"
          json-filename: "${{ env.job_name }} (${{ join(matrix.*, ', ') }}) - cluster 2"

      #
      # Cluster1[VinE]<->Cluster2[VinE] tests passed...
      # Downgrade to Cluster1[Downgrade]<->Cluster2[VinE] to ensure downgrade
      # does not break datapath.
      #

      - name: Downgrade Cluster1[Downgrade]
        env:
          KVSTORE_ID: 1
        run: |
          cilium --context ${{ env.contextName1 }} upgrade --reset-values \
            ${{ steps.downgrade-vars.outputs.cilium_install_defaults }} \
            ${{ steps.vars.outputs.cilium_install_defaults }} \
            ${{ steps.kvstore.outputs.cilium_install_kvstore }} \
            ${{ steps.clustermesh-vars.outputs.cilium_install_cluster1 }}

      - name: Wait for cluster mesh status to be ready
        run: |
          cilium --context ${{ env.contextName1 }} status --wait --wait-duration=10m
          cilium --context ${{ env.contextName2 }} status --wait --wait-duration=10m
          cilium --context ${{ env.contextName1 }} clustermesh status --wait --wait-duration=5m
          cilium --context ${{ env.contextName2 }} clustermesh status --wait --wait-duration=5m

      - name: Run connectivity test - Cluster1[Downgrade]<->Cluster2[VinE] (Cluster1 Downgraded) (${{ join(matrix.*, ', ') }})
        run: |
          cilium --context ${{ env.contextName1 }} connectivity test \
            --multi-cluster=${{ env.contextName2 }} \
            ${{ steps.vars.outputs.connectivity_test_defaults }} \
            --include-conn-disrupt-test \
            --conn-disrupt-test-restarts-path "./cilium-conn-disrupt-restarts" \
            --conn-disrupt-test-xfrm-errors-path "./cilium-conn-disrupt-xfrm-errors" \
            --junit-file "cilium-junits/${{ env.job_name }} - pre-upgrade (${{ join(matrix.*, ', ') }}).xml" \
            --junit-property github_job_step="Run tests pre-upgrade (${{ join(matrix.*, ', ') }})"

          # Create pods which establish long lived connections. They will be used by
          # subsequent connectivity tests with --include-conn-disrupt-test to catch any
          # interruption in such flows.
          cilium --context ${{ env.contextName1 }} connectivity test \
            --multi-cluster=${{ env.contextName2 }} --hubble=false \
            --include-conn-disrupt-test --conn-disrupt-test-setup \
            --conn-disrupt-test-restarts-path "./cilium-conn-disrupt-restarts" \
            --conn-disrupt-test-xfrm-errors-path "./cilium-conn-disrupt-xfrm-errors" \
            --conn-disrupt-dispatch-interval 0ms

      - name: Post-test information gathering
        if: ${{ !success() && steps.install-cilium-cluster1.outcome != 'skipped' }}
        run: |
          cilium --context ${{ env.contextName1 }} status
          cilium --context ${{ env.contextName1 }} clustermesh status
          cilium --context ${{ env.contextName2 }} status
          cilium --context ${{ env.contextName2 }} clustermesh status

          kubectl config use-context ${{ env.contextName1 }}
          kubectl get pods --all-namespaces -o wide
          cilium sysdump --output-filename cilium-sysdump-context1-final-${{ join(matrix.*, '-') }}

          kubectl config use-context ${{ env.contextName2 }}
          kubectl get pods --all-namespaces -o wide
          cilium sysdump --output-filename cilium-sysdump-context2-final-${{ join(matrix.*, '-') }}

          if [ "${{ matrix.external-kvstore }}" == "true" ]; then
            for i in {1..2}; do
              echo
              echo "# Retrieving logs from kvstore$i docker container"
              docker logs kvstore$i
            done
          fi
        shell: bash {0} # Disable default fail-fast behaviour so that all commands run independently


      - name: Upload artifacts
        # if: ${{ !success() }}
        if: ${{ always() }}
        uses: actions/upload-artifact@b4b15b8c7c6ac21ea08fcf65892d2ee8f75cf882 # v4.4.3
        with:
          name: cilium-sysdumps-${{ matrix.name }}
          path: cilium-sysdump-*.zip

      - name: Upload JUnits [junit]
        if: ${{ always() }}
        uses: actions/upload-artifact@b4b15b8c7c6ac21ea08fcf65892d2ee8f75cf882 # v4.4.3
        with:
          name: cilium-junits-${{ matrix.name }}
          path: cilium-junits/*.xml

      - name: Upload features tested
        if: ${{ always() }}
        uses: actions/upload-artifact@b4b15b8c7c6ac21ea08fcf65892d2ee8f75cf882 # v4.4.3
        with:
          name: features-tested-${{ matrix.name }}
          path: ${{ env.job_name }}*.json

      - name: Publish Test Results As GitHub Summary
        if: ${{ always() }}
        uses: aanm/junit2md@332ebf0fddd34e91b03a832cfafaa826306558f9 # v0.0.3
        with:
          junit-directory: "cilium-junits"

  merge-upload:
    if: ${{ always() }}
    name: Merge and Upload Artifacts
    runs-on: ubuntu-24.04
    needs: upgrade-and-downgrade
    steps:
      - name: Checkout context ref (trusted)
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: ${{ inputs.context-ref || github.sha }}
          persist-credentials: false
      - name: Merge Sysdumps
        uses: ./.github/actions/merge-artifacts
        with:
          name: cilium-sysdumps
          pattern: cilium-sysdumps-*
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: Merge JUnits
        uses: ./.github/actions/merge-artifacts
        with:
          name: cilium-junits
          pattern: cilium-junits-*
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: Merge Features tested
        uses: ./.github/actions/merge-artifacts
        with:
          name: features-tested
          pattern: features-tested-*
          token: ${{ secrets.GITHUB_TOKEN }}

  commit-status-final:
    if: ${{ always() }}
    name: Commit Status Final
    needs: upgrade-and-downgrade
    runs-on: ubuntu-24.04
    steps:
      - name: Set final commit status
        uses: myrotvorets/set-commit-status-action@3730c0a348a2ace3c110851bed53331bc6406e9f # v2.0.1
        with:
          sha: ${{ inputs.SHA || github.sha }}
          status: ${{ needs.upgrade-and-downgrade.result }}
